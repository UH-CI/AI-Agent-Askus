{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from glob import glob\n",
    "from html2text import HTML2Text\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ai-agent-askus/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/opt/miniconda3/envs/ai-agent-askus/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name='paraphrase-MiniLM-L6-v2')\n",
    "# semantic_chunker = SemanticChunker(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(collection_name=\"faq\", persist_directory=\"./db\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faq_html_parser(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    question = soup.find(id=\"kb_article_question\")\n",
    "    answer = soup.find(id=\"kb_article_text\")\n",
    "\n",
    "    if not question or not answer:\n",
    "        return None\n",
    "    \n",
    "    return f\"question: {question.text}\\n\\nanswer: {answer.text}\"\n",
    "\n",
    "class HTMLDirectoryLoader(BaseLoader):\n",
    "    def __init__(self, dir_path: str, html_parser):\n",
    "        self.html_parser = html_parser\n",
    "        self.dir_path = dir_path\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        for path in glob(f'{self.dir_path}/*.html'):\n",
    "            with open(path, \"r\") as f:\n",
    "                html_file = f.read()\n",
    "            extracted = self.html_parser(html_file)\n",
    "\n",
    "            if not extracted:\n",
    "                continue\n",
    "            \n",
    "            yield Document(page_content=extracted, metadata={\"source\": path})\n",
    "\n",
    "faq_html_loader = HTMLDirectoryLoader(\"../web-scraper/faq-archive\", faq_html_parser)\n",
    "documents = faq_html_loader.lazy_load()\n",
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Current News: Mānoa: New zero-interest loan program for UH engineering students\n",
      "For more information, visit this url: https://manoa.hawaii.edu/news/article.php?aId=13339' metadata={'source': 'https://manoa.hawaii.edu/news/article.php?aId=13339', 'title': 'Mānoa: New zero-interest loan program for UH engineering students'}\n"
     ]
    }
   ],
   "source": [
    "class ManoaNewsLoader(BaseLoader):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        base_url = \"https://manoa.hawaii.edu/news/archive.php\"\n",
    "        response = requests.get(base_url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        news = soup.find(\"news\")\n",
    "        latest = news.find(\"div\")\n",
    "        title = latest.text\n",
    "        news_url = base_url.replace(\"archive\", \"article\") + \"?aId=13339\"\n",
    "\n",
    "        # expanded_response = requests.get(news_url)\n",
    "\n",
    "        # if response.status_code != 200:\n",
    "        #     return []\n",
    "        \n",
    "        # soup = BeautifulSoup(expanded_response.content, 'html.parser')\n",
    "        # yield soup\n",
    "        # expanded_news = soup.find(\"news\")\n",
    "\n",
    "        yield Document(page_content=f\"Current News: {title}\\nFor more information, visit this url: {news_url}\", metadata={\"source\": news_url, \"title\": title})\n",
    "        \n",
    "\n",
    "manoa_news_loader = ManoaNewsLoader()\n",
    "for doc in manoa_news_loader.lazy_load():\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise. DO NOT mention the context, users do not see it.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The phone number for ITS (Information Technology Services) at UH is 808-956-6033.\n",
      "According to the context provided earlier, the phone numbers mentioned are:\n",
      "\n",
      "* Directory Assistance (1-808-555-1212) - $2.49 per call\n",
      "* Directory Assistance (9+00) - $7.95 per call\n",
      "\n",
      "Let me know if you need any further assistance!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input()\n",
    "    if not user_input:\n",
    "        break\n",
    "    answer = conversational_rag_chain.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"1\"}\n",
    "        },\n",
    "    )[\"answer\"]\n",
    "\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-askus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
