{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'dunzhang/stella_en_400M_v5'\n",
    "model_kwargs = {'device': 'cuda', \"trust_remote_code\": True}\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"faq\",\n",
    "    persist_directory=\"./db\",\n",
    "    embedding_function=embedding_model,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.reset_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.HTMLDirectory import HTMLDirectoryLoader\n",
    "\n",
    "def faq_html_parser(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    question = soup.find(id=\"kb_article_question\")\n",
    "    answer = soup.find(id=\"kb_article_text\")\n",
    "\n",
    "    if not question or not answer:\n",
    "        return None\n",
    "    \n",
    "    qa = f\"{question.text.strip()}\\n{answer.text.strip()}\"\n",
    "    removed_repeating_newlines = re.sub(r'\\n{2,}', '\\n', qa)\n",
    "\n",
    "    return removed_repeating_newlines\n",
    "\n",
    "faq_html_loader = HTMLDirectoryLoader(\"../web-scraper/faq-archive\", faq_html_parser)\n",
    "faq_documents = list(faq_html_loader.lazy_load())\n",
    "faq_split_documents = text_splitter.split_documents(faq_documents)\n",
    "vector_store.add_documents(faq_split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.JSONFile import JSONFileLoader\n",
    "\n",
    "def json_parser(d):\n",
    "    return {\n",
    "        \"page_content\": d[\"extracted\"],\n",
    "        \"metadata\" : {\"source\": d[\"url\"]}\n",
    "    }\n",
    "\n",
    "json_file_loader = JSONFileLoader(\"../web-scraper/data/urls.json\", json_parser)\n",
    "json_documents = json_file_loader.lazy_load()\n",
    "json_split_documents = text_splitter.split_documents(json_documents)\n",
    "vector_store.add_documents(json_split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"gemma2\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "from langchain.globals import set_verbose\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "set_verbose(True)\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={'k': 2}\n",
    ")\n",
    "\n",
    "combined_retriever = EnsembleRetriever(retrievers=[retriever, ])\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=combined_retriever\n",
    ")\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question. \"\n",
    "    \"just reformulate it if needed and otherwise return it as is. \"\n",
    "    \"if there is no chat history, return the input as is. \"\n",
    "    \"if the input is a greeting, return the input as is. \"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, combined_retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"Your name is Hoku. You are an assistant for answering questions about UH Manoa.\"\n",
    "    \"Answer the question given ONLY the provided context.\\n\"\n",
    "    \"If the answer DOES NOT appear in the context, say 'I'm sorry I don't know the answer to that'.\\n\"\n",
    "    \"Use three sentences maximum and keep the answer concise and speak nicely.\\n\"\n",
    "    \"DO NOT mention the context, users do not see it.\"\n",
    "    \"if the user greets you, greet them back nicely\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"context:{context}\\n\\nquestion: {input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "sources_examples = [\n",
    "    {\"input\": \"Hi Hoku!\", \"output\": \"no\"},\n",
    "    {\"input\": \"How are you?\", \"output\": \"no\"},\n",
    "    {\"input\": \"What is duo mobile used for?\", \"output\": \"yes\"},\n",
    "    {\"input\": \"what specs should i have for a mac laptop?\", \"output\": \"yes\"},\n",
    "    {\"input\": \"Thank you!\", \"output\": \"no\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=sources_examples,\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Your job is to classify a user input as needing sources 'yes' or not needing sources 'no'.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def requires_source(inp: dict):\n",
    "    chain = final_prompt | llm\n",
    "    return \"yes\" in chain.invoke(inp).content.lower()\n",
    "\n",
    "\n",
    "def add_sources_to_response_if_needed(inp: dict) -> dict:\n",
    "    if not requires_source({\"input\" : inp[\"input\"]}):\n",
    "        return inp\n",
    "    \n",
    "    sources_text = \"\\n\".join(list(set(doc.metadata[\"source\"] for doc in inp['context'])))\n",
    "    inp[\"answer\"] = f\"{inp['answer'].strip()}\\n\\nFor more information, check out these links\\n{sources_text}\"\n",
    "    return inp\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "conversational_rag_chain_with_sources = conversational_rag_chain | add_sources_to_response_if_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input()\n",
    "    print(user_input)\n",
    "    \n",
    "    if not user_input:\n",
    "        break\n",
    "    \n",
    "    answer = conversational_rag_chain_with_sources.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"1\"},\n",
    "            # 'callbacks': [ConsoleCallbackHandler()]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(answer[\"answer\"])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
